{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\python27\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\python27\\lib\\site-packages (from lightgbm) (1.16.6)\n",
      "Requirement already satisfied: scipy in c:\\python27\\lib\\site-packages (from lightgbm) (1.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\python27\\lib\\site-packages (from lightgbm) (0.20.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel in c:\\python27\\lib\\site-packages (0.34.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "!pip install wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>City</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-08</td>\n",
       "      <td>Florence</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>Florence</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>Florence</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2020-04-14</td>\n",
       "      <td>Florence</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>Florence</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      City  median\n",
       "0  2020-03-08  Florence    50.0\n",
       "1  2020-03-18  Florence    59.0\n",
       "2  2020-03-31  Florence    21.0\n",
       "3  2020-04-14  Florence    42.0\n",
       "4  2020-03-03  Florence    21.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air = pd.read_csv(\"../Resources/airdata3.csv\")\n",
    "air = air.drop(columns = {\"Unnamed: 0\", \"Country\", \"count\", \"min\", \"variance\", \"Specie\", \"max\"})\n",
    "air.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>City</th>\n",
       "      <th>median</th>\n",
       "      <th>DatePriorMed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>Milan</td>\n",
       "      <td>157.0</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>54.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>Florence</td>\n",
       "      <td>74.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>160.0</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>London</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date      City  median  DatePriorMed\n",
       "306  2020-01-11     Milan   157.0         155.0\n",
       "638  2020-01-11  Santiago    54.0          50.0\n",
       "62   2020-01-11  Florence    74.0         109.0\n",
       "712  2020-01-11    Mumbai   160.0         155.0\n",
       "862  2020-01-11    London    28.0          33.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new thing:\n",
    "data3 = air.copy()\n",
    "data3 = data3.sort_values(by = [\"Date\"])\n",
    "data3['DatePriorMed'] = data3.groupby(['City'])['median'].shift()\n",
    "data3 = data3.dropna()\n",
    "\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>median</th>\n",
       "      <th>DatePriorMed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>157.0</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>54.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>74.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>160.0</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     median  DatePriorMed\n",
       "306   157.0         155.0\n",
       "638    54.0          50.0\n",
       "62     74.0         109.0\n",
       "712   160.0         155.0\n",
       "862    28.0          33.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data3.copy()\n",
    "df = df.drop(columns = {'Date','City'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(702,)\n",
      "[157.  54.  74. 160.  28. 104.  83. 117.  42.  95.  68.  63.  21. 165.\n",
      " 134.  58.  56. 147. 152.  87.  46. 147. 117.  21.  55.  95.  65.  74.\n",
      "  25. 175. 158.  59. 102. 101.  57. 122.  79. 169.  85. 104.  28. 134.\n",
      "  54. 156. 122. 160. 117.  90.  59.  99.  29.  93. 152.  99. 130. 114.\n",
      " 154.  70. 159. 107.  25.  65. 104. 104. 137. 162. 287.  74.  71.  38.\n",
      "  56.  95.  67. 171.  46.  46. 165.  73.  97.  91.  59. 169.  59.  62.\n",
      "  78. 168.  30.  70.  38.  25. 168. 102.  65.  86. 122.  53.  65. 166.\n",
      "  77. 109.  85. 163. 164.  82.  83. 155.  87.  64.  55.  97. 132.  59.\n",
      "  95. 165.  73. 164. 155.  93. 160. 109.  89. 159.  63. 157.  50.  58.\n",
      "  68.  57. 155.  72. 196. 167. 107.  70.  56.  63.  85. 230.  35.  52.\n",
      "  57. 171.  80. 158.  21. 124.  65. 113. 232.  32.  56. 161.  70.  37.\n",
      " 151.  42.  21.  30.  70. 132.  57. 248.  33. 139.  70. 154.  52.  65.\n",
      "  60.  53.  42.  65.  65.  53.  25. 142. 134.  63. 141.  99.  18.  75.\n",
      "  74. 152. 144. 153. 154.  57.  61. 109. 159.  61.  75. 154. 154.  31.\n",
      "  30.  42. 160.  91.  65.  57.  99.  78.  38. 154.  38. 161.  42.  50.\n",
      " 107.  35.  38.  65.  59. 153.  61.  46. 163.  35.  30.  78.  30. 153.\n",
      "  42.  17.  52.  59.  40.  21.  78. 163.  34. 153.  68. 126.  46. 119.\n",
      "  30.  59.  65.  82.  30.  61.  63.  72. 160.  69.  63.  95.  53. 127.\n",
      "  33. 124. 158.  72.  34.  60. 158.  91.  72.  97.  25. 191.  59.  89.\n",
      "  80. 157.  50. 155. 194. 107.  26.  65.  61. 142.  50. 151.  50. 155.\n",
      "  78.  80.  50.  70.  28. 247.  68.  38. 155.  24.  21. 266.  57.  75.\n",
      "  86.  78.  46.  62.  46.  55.  70. 158.  25.  78. 261.  59.  41.  91.\n",
      "  64.  38.  76.  46. 153.  68.  34.  77.  25.  53.  57.  61.  20. 144.\n",
      "  61. 129.  22.  78.  16.  50. 151.  68. 163.  17.  62.  50.  43.  25.\n",
      " 117. 173.  30.  74.  55.  63. 122.  45.  51.  25.  46.  61.  65. 168.\n",
      "  63.  55.  82.  61.  30. 107. 168. 152.  55.  65.  61. 102.  18.  53.\n",
      "  59. 179.  72.  57. 153. 115.  47.  59. 177.  99.  70.  31.  72. 112.\n",
      " 112.  30.  78.  69. 114.  38.  70. 160.  66.  66.  62. 107.  53.  78.\n",
      " 162. 112.  20.  95.  18. 168. 152. 107. 104.  57.  55.  59.  70. 119.\n",
      "  17.  61. 116.  63.  38.  57. 157. 142.  89. 132.  34.  89.  64. 116.\n",
      "  34.  56.  21.  30.  54.  25.  25.  59.  89.  25. 151.  78.  42.  42.\n",
      "  30. 151.  25.  63.  42. 163.  58.  29. 155.  53. 159.  77.  70.  61.\n",
      "  46.  54.  42.  23. 124.  63.  80.  73.  30.  38.  21.  68.  58.  62.\n",
      "  78. 129. 137.  76.  30.  41.  58.  68.  70. 118.  21.  34.  51.  85.\n",
      "  69.  38.  38. 103.  30.  72. 132.  97.  30.  33.  65.  75.  42. 102.\n",
      "  38.  77.  68.  39.  25. 112.  30.  74.  63. 165.  72.  48.  34.  25.\n",
      "  80.  95. 155.  82.  71.  92.  25.  76.  60.  59.  50.  57. 170.  30.\n",
      "  25.  53.  38. 107.  57. 114.  76.  59. 147.  50.  53.  50.  82.  38.\n",
      "  64.  91.  72.  15.  65.  85.  50.  61.  99.  57.  66.  58.  18.  38.\n",
      "  78.  58.  53.  70.  76. 122.  61.  26.  53.  81.  49.  31. 142.  25.\n",
      "  59.  96.  38.  56.  25.  72.  48.  97.  57.  25. 129.  53.  42.  22.\n",
      "  25.  53. 141.  65.  25.  85.  87.  35. 117.  72.  34.  70.  73.  57.\n",
      "  54. 152.  68.  97.  56.  70.  57.  99. 127.  30. 161.  59. 107.  32.\n",
      " 112.  60.  78. 130. 132.  76.  25.  78.  89.  57.  42.  84.  57.  93.\n",
      "  61. 102.  91.  65.  93.  63.  67.  72.  25.  60.  60.  82.  67.  65.\n",
      "  70.  82.  29. 112.  78.  64.  82. 142.  53.  66.  40.  68.  85.  52.\n",
      "  44. 119.  48.  87.  25.  46.  42.  82.  59.  42. 104.  57.  68.  51.\n",
      " 107.  38.  34.  99.  95. 179.  55.  65.  25.  39.  90.  34.  59.  51.\n",
      "  21.  17.  55.  85.  30.  79.  82.  63.  60.  46.  59.  38.  78.  70.\n",
      "  21.  84.  17.  70.  42.  97.  76.  91.  46.  46.  58.  30.  42.  77.\n",
      "  65.  74.]\n",
      "[155.  50. 109. 155.  33.  82.  82. 109.  87. 117.  42.  74.  28. 160.\n",
      " 157.  83.  54. 104. 134.  63.  68. 165. 147.  21.  56.  95.  58.  65.\n",
      "  21. 117. 152.  46.  95. 147.  55.  87.  74. 175. 101. 102.  25.  59.\n",
      "  57. 158. 122. 156. 104.  85.  54. 134.  28.  79. 169. 122.  90. 152.\n",
      " 160.  59.  99. 117.  29.  93.  99. 104. 114. 130. 159. 107.  70.  25.\n",
      "  65. 154.  71. 137. 104.  38. 162.  56.  95. 287.  74. 165.  97.  67.\n",
      "  46. 171.  91.  73.  59.  46. 169.  59.  38.  78.  30.  25.  62. 168.\n",
      "  70. 102.  77. 168. 166.  53.  86. 122.  65.  65.  64.  87. 109.  83.\n",
      "  82. 163.  85. 164. 155.  95. 165.  97. 164. 132.  73. 155.  55.  59.\n",
      "  93.  50. 159. 109. 157. 160.  89.  58.  63.  72. 107. 196.  56.  70.\n",
      "  57. 167.  68. 155.  52.  85.  80. 171. 230.  35.  57. 158.  63.  32.\n",
      " 124.  70.  21.  65. 113. 161.  56. 232.  21. 151.  70. 248.  57. 132.\n",
      "  37.  42.  30.  60.  53.  52.  33. 139. 154.  42.  70.  65.  25.  65.\n",
      "  65. 134.  99. 141. 142.  63.  53.  74. 153.  61.  75. 144. 154.  18.\n",
      " 152.  57. 159. 154. 109.  61.  30.  75.  42. 154.  31. 160.  57.  38.\n",
      "  91.  38.  99.  78.  65. 154.  65.  59. 161.  42.  38. 107.  35. 153.\n",
      "  50.  46.  61.  30.  35.  42.  30. 153.  78. 163.  34. 153.  40.  78.\n",
      "  17.  52.  59. 163.  21.  82.  30.  59. 119.  65.  30.  68.  46. 126.\n",
      "  72.  95. 160.  63.  69.  53. 127.  63.  61.  72.  34. 158.  60.  72.\n",
      "  91. 158.  33. 124. 191.  89.  25.  50.  59. 155.  80. 157.  97. 151.\n",
      "  65. 107.  50. 142.  26. 194.  61.  50. 155.  28.  70. 247.  50.  68.\n",
      "  78.  80.  38.  75.  46.  86.  78. 155.  24.  21. 266.  57.  25.  78.\n",
      "  62.  46. 261.  55. 158.  70.  59.  64.  76.  68.  38.  34.  41. 153.\n",
      "  46.  91.  61.  61.  20.  53. 129.  57. 144.  25.  77.  62.  22.  17.\n",
      " 151. 163.  16.  78.  50.  68. 117.  43.  50.  30.  63.  25.  55. 173.\n",
      "  74.  45.  65.  51.  25. 122. 168.  61.  46.  63. 107.  82.  30.  65.\n",
      "  55. 152.  61.  55. 168. 153.  57.  53. 179.  61.  59.  18.  72. 102.\n",
      "  99. 177.  70.  47. 112.  31.  59. 115.  72.  69.  66. 112.  78.  70.\n",
      " 160.  30.  38. 114.  20. 162. 112.  95. 107.  78.  53.  62.  66. 104.\n",
      "  18.  59. 168.  70.  57.  55. 107. 152. 119. 157.  57. 142.  61. 116.\n",
      "  38.  63.  17.  89.  56.  34.  21.  64. 132.  34. 116.  89.  89.  25.\n",
      "  25.  78.  25.  59.  30. 151.  54.  25. 163.  42. 151.  63.  42.  42.\n",
      "  30.  58.  46.  29. 155.  54.  53.  77.  61. 159.  23.  38.  63.  73.\n",
      "  80. 124.  68.  78.  42.  21.  58.  70.  62. 129.  30.  30.  58.  76.\n",
      "  70.  41. 137. 118.  21.  68.  85. 103.  34.  51.  72.  69.  30.  38.\n",
      "  38.  75.  97.  38.  42. 132.  30.  65.  33. 102.  63.  39.  30.  25.\n",
      "  74. 112. 165.  68.  77.  82.  48.  95.  72.  71.  25.  80. 155.  34.\n",
      "  25.  50.  30.  92.  59.  76.  57.  60. 170.  53.  38.  59. 107. 147.\n",
      "  57. 114.  76.  25.  53.  38.  72.  50.  91.  50.  64.  82.  15.  85.\n",
      "  57.  66.  65.  50.  61.  99.  58.  18.  78.  61.  58.  26. 122.  38.\n",
      "  53.  76.  70.  96.  25.  38.  49.  81.  59.  31. 142.  53.  57.  25.\n",
      "  25.  48.  97.  72.  53.  56. 129.  22.  87.  25.  25.  65.  85.  42.\n",
      "  53. 141.  57.  70.  54.  73.  34.  72. 117.  35. 152.  57.  97.  30.\n",
      "  99.  56.  68. 161. 127.  70. 112.  78. 107.  60.  32. 130.  59. 132.\n",
      "  76.  89.  93.  57.  25.  84.  61.  78.  42.  57.  60.  72.  63.  65.\n",
      " 102.  93.  25.  91.  67.  67.  82. 112.  65.  78.  29.  70.  82.  60.\n",
      "  52. 142.  40.  85.  53.  82.  68.  64.  66.  44. 119.  42.  59.  48.\n",
      "  82.  25.  46.  87. 104.  99.  57.  51.  34.  42. 107.  38.  68.  39.\n",
      " 179.  25.  55.  90.  34.  65.  95.  59.  63.  30.  51.  82.  79.  55.\n",
      "  21.  85.  17.  59.  38.  46.  70.  84.  17.  78.  60.  21.  42.  70.\n",
      "  46.  76.]\n"
     ]
    }
   ],
   "source": [
    "#70%train,10% val, 20% test\n",
    "\n",
    "train_i = int(len(df.index) * 0.7)\n",
    "val_i = int(len(df.index)*0.8)\n",
    "\n",
    "train_x=df['median'].iloc[:train_i].to_numpy()\n",
    "train_y=df['DatePriorMed'].iloc[:train_i].to_numpy()\n",
    "\n",
    "val_x=df['median'].iloc[train_i:val_i].to_numpy()\n",
    "val_y=df['DatePriorMed'].iloc[train_i:val_i].to_numpy()\n",
    "\n",
    "test_x=df['median'].iloc[val_i:].to_numpy()\n",
    "test_y=df['DatePriorMed'].iloc[val_i:].to_numpy()\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_x)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 6)                 12        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 45\n",
      "Trainable params: 45\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(6, activation = 'relu', input_shape=(1,)),\n",
    "    tf.keras.layers.Dense(4, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "epochs = 100\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'mean_squared_error',\n",
    "              metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 702 samples, validate on 101 samples\n",
      "Epoch 1/100\n",
      "702/702 [==============================] - 0s 615us/sample - loss: 12547.7421 - acc: 0.0000e+00 - val_loss: 6785.5767 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "702/702 [==============================] - 0s 40us/sample - loss: 9320.3993 - acc: 0.0000e+00 - val_loss: 4975.8325 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 6799.6399 - acc: 0.0000e+00 - val_loss: 3524.0664 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 4807.5878 - acc: 0.0000e+00 - val_loss: 2441.0554 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "702/702 [==============================] - 0s 36us/sample - loss: 3317.8676 - acc: 0.0000e+00 - val_loss: 1672.8417 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "702/702 [==============================] - 0s 36us/sample - loss: 2313.1463 - acc: 0.0000e+00 - val_loss: 1242.0644 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "702/702 [==============================] - 0s 36us/sample - loss: 1814.5575 - acc: 0.0000e+00 - val_loss: 1003.9315 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 1468.4477 - acc: 0.0000e+00 - val_loss: 823.9665 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "702/702 [==============================] - 0s 36us/sample - loss: 1208.6048 - acc: 0.0000e+00 - val_loss: 698.0007 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "702/702 [==============================] - 0s 35us/sample - loss: 1026.9417 - acc: 0.0000e+00 - val_loss: 616.8685 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 907.6830 - acc: 0.0000e+00 - val_loss: 570.3481 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 836.3186 - acc: 0.0000e+00 - val_loss: 546.5604 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 797.4699 - acc: 0.0000e+00 - val_loss: 535.7266 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 776.9495 - acc: 0.0000e+00 - val_loss: 532.3953 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "702/702 [==============================] - 0s 36us/sample - loss: 768.5099 - acc: 0.0000e+00 - val_loss: 532.2722 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "702/702 [==============================] - ETA: 0s - loss: 821.0909 - acc: 0.0000e+0 - 0s 36us/sample - loss: 763.6440 - acc: 0.0000e+00 - val_loss: 533.0174 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 762.0006 - acc: 0.0000e+00 - val_loss: 533.9380 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 761.4139 - acc: 0.0000e+00 - val_loss: 534.5348 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "702/702 [==============================] - 0s 36us/sample - loss: 761.4348 - acc: 0.0000e+00 - val_loss: 534.8290 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 760.8989 - acc: 0.0000e+00 - val_loss: 535.8104 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 761.2394 - acc: 0.0000e+00 - val_loss: 535.2305 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "702/702 [==============================] - 0s 40us/sample - loss: 760.7127 - acc: 0.0000e+00 - val_loss: 535.6072 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 761.2803 - acc: 0.0000e+00 - val_loss: 537.0695 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 760.5551 - acc: 0.0000e+00 - val_loss: 536.4106 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "702/702 [==============================] - 0s 35us/sample - loss: 761.0487 - acc: 0.0000e+00 - val_loss: 534.6532 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "702/702 [==============================] - 0s 36us/sample - loss: 761.1417 - acc: 0.0000e+00 - val_loss: 534.7548 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "702/702 [==============================] - 0s 36us/sample - loss: 760.2377 - acc: 0.0000e+00 - val_loss: 535.8128 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 760.6085 - acc: 0.0000e+00 - val_loss: 535.8819 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "702/702 [==============================] - 0s 40us/sample - loss: 760.4827 - acc: 0.0000e+00 - val_loss: 535.4696 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 760.0787 - acc: 0.0000e+00 - val_loss: 535.7352 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 760.3519 - acc: 0.0000e+00 - val_loss: 535.7042 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "702/702 [==============================] - ETA: 0s - loss: 399.5190 - acc: 0.0000e+0 - 0s 37us/sample - loss: 760.3387 - acc: 0.0000e+00 - val_loss: 534.9036 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 760.0906 - acc: 0.0000e+00 - val_loss: 536.0300 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "702/702 [==============================] - 0s 35us/sample - loss: 760.5470 - acc: 0.0000e+00 - val_loss: 534.1127 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "702/702 [==============================] - ETA: 0s - loss: 557.0443 - acc: 0.0000e+0 - 0s 36us/sample - loss: 760.3021 - acc: 0.0000e+00 - val_loss: 535.5409 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 759.5258 - acc: 0.0000e+00 - val_loss: 535.0902 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "702/702 [==============================] - 0s 36us/sample - loss: 759.9230 - acc: 0.0000e+00 - val_loss: 534.1513 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 759.5903 - acc: 0.0000e+00 - val_loss: 534.8802 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 759.4006 - acc: 0.0000e+00 - val_loss: 534.4892 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 759.4059 - acc: 0.0000e+00 - val_loss: 534.9101 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 759.2254 - acc: 0.0000e+00 - val_loss: 534.1788 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 759.2084 - acc: 0.0000e+00 - val_loss: 534.0841 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 758.6771 - acc: 0.0000e+00 - val_loss: 534.5791 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "702/702 [==============================] - ETA: 0s - loss: 738.2706 - acc: 0.0000e+0 - 0s 37us/sample - loss: 759.0824 - acc: 0.0000e+00 - val_loss: 534.3272 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 758.6091 - acc: 0.0000e+00 - val_loss: 534.1135 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 759.0775 - acc: 0.0000e+00 - val_loss: 533.2896 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 758.5528 - acc: 0.0000e+00 - val_loss: 533.7292 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 758.6178 - acc: 0.0000e+00 - val_loss: 533.9256 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 758.2319 - acc: 0.0000e+00 - val_loss: 533.6630 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 757.9946 - acc: 0.0000e+00 - val_loss: 533.3419 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 758.1595 - acc: 0.0000e+00 - val_loss: 533.9760 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "702/702 [==============================] - 0s 40us/sample - loss: 757.7547 - acc: 0.0000e+00 - val_loss: 533.0181 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "702/702 [==============================] - 0s 40us/sample - loss: 757.8347 - acc: 0.0000e+00 - val_loss: 533.8536 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 0s 38us/sample - loss: 757.3051 - acc: 0.0000e+00 - val_loss: 533.0157 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 757.2791 - acc: 0.0000e+00 - val_loss: 532.7459 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "702/702 [==============================] - 0s 40us/sample - loss: 757.6346 - acc: 0.0000e+00 - val_loss: 533.1798 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "702/702 [==============================] - 0s 36us/sample - loss: 757.0294 - acc: 0.0000e+00 - val_loss: 532.3943 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 757.0228 - acc: 0.0000e+00 - val_loss: 532.2892 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 757.4157 - acc: 0.0000e+00 - val_loss: 531.9023 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 756.4664 - acc: 0.0000e+00 - val_loss: 533.3270 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 756.6880 - acc: 0.0000e+00 - val_loss: 534.0838 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 756.9818 - acc: 0.0000e+00 - val_loss: 533.7921 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 756.5377 - acc: 0.0000e+00 - val_loss: 533.0780 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 755.9296 - acc: 0.0000e+00 - val_loss: 532.0867 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 756.0366 - acc: 0.0000e+00 - val_loss: 530.7834 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 756.3991 - acc: 0.0000e+00 - val_loss: 531.4717 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 755.7712 - acc: 0.0000e+00 - val_loss: 532.0196 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 755.5095 - acc: 0.0000e+00 - val_loss: 531.0700 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 755.5230 - acc: 0.0000e+00 - val_loss: 531.0847 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 755.1131 - acc: 0.0000e+00 - val_loss: 531.8765 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 755.2231 - acc: 0.0000e+00 - val_loss: 530.9525 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 754.9201 - acc: 0.0000e+00 - val_loss: 530.7433 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "702/702 [==============================] - 0s 36us/sample - loss: 754.5994 - acc: 0.0000e+00 - val_loss: 532.2634 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "702/702 [==============================] - 0s 36us/sample - loss: 755.3178 - acc: 0.0000e+00 - val_loss: 531.6088 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 754.7429 - acc: 0.0000e+00 - val_loss: 530.9816 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 755.3429 - acc: 0.0000e+00 - val_loss: 529.9810 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 754.7857 - acc: 0.0000e+00 - val_loss: 532.5126 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 756.0670 - acc: 0.0000e+00 - val_loss: 531.1567 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "702/702 [==============================] - 0s 43us/sample - loss: 754.5897 - acc: 0.0000e+00 - val_loss: 528.0409 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "702/702 [==============================] - 0s 47us/sample - loss: 754.1155 - acc: 0.0000e+00 - val_loss: 531.0965 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "702/702 [==============================] - 0s 45us/sample - loss: 754.4356 - acc: 0.0000e+00 - val_loss: 529.9909 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "702/702 [==============================] - 0s 46us/sample - loss: 753.8722 - acc: 0.0000e+00 - val_loss: 531.8516 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "702/702 [==============================] - 0s 45us/sample - loss: 753.8429 - acc: 0.0000e+00 - val_loss: 530.7765 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "702/702 [==============================] - 0s 47us/sample - loss: 753.9697 - acc: 0.0000e+00 - val_loss: 531.8009 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "702/702 [==============================] - 0s 43us/sample - loss: 755.0829 - acc: 0.0000e+00 - val_loss: 527.5451 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "702/702 [==============================] - 0s 45us/sample - loss: 756.7020 - acc: 0.0000e+00 - val_loss: 530.4068 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "702/702 [==============================] - 0s 45us/sample - loss: 752.4145 - acc: 0.0000e+00 - val_loss: 529.0631 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 752.2269 - acc: 0.0000e+00 - val_loss: 529.3436 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 752.9309 - acc: 0.0000e+00 - val_loss: 527.4894 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 753.3884 - acc: 0.0000e+00 - val_loss: 530.8687 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 751.9633 - acc: 0.0000e+00 - val_loss: 528.1642 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 751.9673 - acc: 0.0000e+00 - val_loss: 526.7198 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 751.6638 - acc: 0.0000e+00 - val_loss: 529.8906 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 751.4700 - acc: 0.0000e+00 - val_loss: 528.5954 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 751.4309 - acc: 0.0000e+00 - val_loss: 528.8231 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "702/702 [==============================] - 0s 44us/sample - loss: 752.2847 - acc: 0.0000e+00 - val_loss: 529.7690 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "702/702 [==============================] - 0s 43us/sample - loss: 750.9913 - acc: 0.0000e+00 - val_loss: 527.5224 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 750.6917 - acc: 0.0000e+00 - val_loss: 527.6509 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "702/702 [==============================] - 0s 37us/sample - loss: 750.4348 - acc: 0.0000e+00 - val_loss: 526.6790 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "702/702 [==============================] - 0s 38us/sample - loss: 750.2777 - acc: 0.0000e+00 - val_loss: 528.4853 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f6b6437dc8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=epochs,\n",
    "    validation_data = (val_x,val_y),\n",
    "    validation_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/201 [==============================] - 0s 40us/sample - loss: 351.6185 - acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y)\n",
    "prediction = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 351.6185423042861\n",
      "R-squared (R2 ): 0.5843838547109963\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean_squared_error and the r-squared value\n",
    "# for the testing data\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "# Use our model to make predictions\n",
    "predicted = model.predict(test_x)\n",
    "\n",
    "# Score the predictions with mse and r2\n",
    "mse = mean_squared_error(test_y, predicted)\n",
    "r2 = r2_score(test_y, predicted)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2 ): {r2}\")\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('whatever.h5')\n",
    "#model.load('whatever.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = soemthing this is today\n",
    "\n",
    "#graph = []\n",
    "#for 100\n",
    "\n",
    "#    predict = model.predict(x)\n",
    "#   \n",
    "#    graph.append(prediction)\n",
    "#   \n",
    "#    x = predict.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1004, 1) (1004, 1)\n"
     ]
    }
   ],
   "source": [
    "# Assign the data to X and y\n",
    "\n",
    "X = data3[[\"DatePriorMed\"]]\n",
    "y = data3[\"median\"].values.reshape(-1, 1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to create training and testing data\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using LinearRegression\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.6739133316665591\n",
      "Testing Score: 0.5686579390080178\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data and calculate the scores for the training and testing data\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "model.fit(X_train, y_train)\n",
    "training_score = model.score(X_train, y_train)\n",
    "testing_score = model.score(X_test, y_test)\n",
    "\n",
    "### END SOLUTION \n",
    "\n",
    "print(f\"Training Score: {training_score}\")\n",
    "print(f\"Testing Score: {testing_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(X_test, y_test,  color='black')\n",
    "plt.plot(X_test, y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW WHAT?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
